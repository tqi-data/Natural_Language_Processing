{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Sentiment Analysis with Python scikit-learn **\n",
    "\n",
    "https://marcobonzanini.com/2015/01/19/sentiment-analysis-with-python-and-scikit-learn/\n",
    "\n",
    "Sentiment Analysis is a field of study which analyses people’s opinions towards entities like products, typically expressed in written forms like on-line reviews. Thanks to the massive popularity of social media which provide a constant source of textual data full of opinions to analyse.\n",
    "\n",
    "This jupyter notebook focuses on one particular application of sentiment analysis: **sentiment classification at the document level**. In other words, given a document (e.g. a review), the task consists in finding out whether it provides a positive or a negative sentiment towards the product being discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set we use is the well-known **Polarity Dataset v2.0**. It is database of **movie reviews**, which contains 2,000 documents, labelled and pre-preprocssed. In particular, there are two labels, positive and negative with 1,000 documents each. Every document has been tokenized and lowercased; each line of a document represents a sentence. This pre-processing takes out most of the work we have to do to get started, so we can focus on the classification problem. Real world data are usually messy and need proper pre-processing before we can make good use of them. All we need to do here is read the files and split the words over white spaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The correct path to the data file needs to be provided. \n",
    "#Under this path, there are two subfolders \"neg/\" and \"pos/\"\n",
    "data_dir = \"txt_sentoken/\"\n",
    "classes = ['neg', 'pos']\n",
    "train_data = []\n",
    "train_labels = []\n",
    "test_data = []\n",
    "test_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the data\n",
    "for curr_class in classes:\n",
    "    # os.path.join means joining path components. It should look like this: txt_sentoken/neg/\n",
    "    dirname = os.path.join(data_dir, curr_class)\n",
    "    # os.listdir means listing all file names in a given directory \n",
    "    for fname in os.listdir(dirname):\n",
    "        # Using \"with\" and \"as\" to open a file, process its contents, and close it.\n",
    "        with open(os.path.join(dirname, fname), 'r') as f:\n",
    "            content = f.read()\n",
    "            # Python string startswith() method\n",
    "            # putting all files with filename starting with 'cv9' to test data set\n",
    "            if fname.startswith('cv9'):\n",
    "                test_data.append(content)\n",
    "                test_labels.append(curr_class)\n",
    "            else:\n",
    "                train_data.append(content)\n",
    "                train_labels.append(curr_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words are used as features. Scikit-learn provides several vectorizers. TF-IDF is one of the most common weighting schemes. The parameters used in this example with the vectorizer are:\n",
    "\n",
    "* min_df=5, discard words appearing in less than 5 documents\n",
    "* max_df=0.8, discard words appering in more than 80% of the documents\n",
    "* sublinear_tf=True, use sublinear weighting\n",
    "* use_idf=True, enable IDF\n",
    "\n",
    "More options are available and the best configuration might depend on your data or on the details of the task you’re facing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create feature vectors\n",
    "vectorizer = TfidfVectorizer(min_df = 5, max_df = 0.8, sublinear_tf=True, use_idf=True)\n",
    "\n",
    "train_vectors = vectorizer.fit_transform(train_data)\n",
    "test_vectors = vectorizer.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform classification with SVM, kernel=rbf (Gaussian)\n",
    "classifier_rbf = svm.SVC()\n",
    "t0 = time.time()\n",
    "classifier_rbf.fit(train_vectors, train_labels)\n",
    "t1 = time.time()\n",
    "prediction_rbf = classifier_rbf.predict(test_vectors)\n",
    "t2 = time.time()\n",
    "time_rbf_train = t1-t0\n",
    "time_rbf_predict = t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform classification with SVM, kernel=linear\n",
    "classifier_linear = svm.SVC(kernel='linear')\n",
    "t0 = time.time()\n",
    "classifier_linear.fit(train_vectors, train_labels)\n",
    "t1 = time.time()\n",
    "prediction_linear = classifier_linear.predict(test_vectors)\n",
    "t2 = time.time()\n",
    "time_linear_train = t1-t0\n",
    "time_linear_predict = t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for SVC(kernel=rbf)\n",
      "Training time: 8.020822s; Prediction time: 0.938096s\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.86      0.75      0.80       100\n",
      "        pos       0.78      0.88      0.83       100\n",
      "\n",
      "avg / total       0.82      0.81      0.81       200\n",
      "\n",
      "Results for SVC(kernel=linear)\n",
      "Training time: 7.193538s; Prediction time: 0.708705s\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.91      0.92      0.92       100\n",
      "        pos       0.92      0.91      0.91       100\n",
      "\n",
      "avg / total       0.92      0.92      0.91       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print results in a nice table\n",
    "print(\"Results for SVC(kernel=rbf)\")\n",
    "print(\"Training time: %fs; Prediction time: %fs\" % (time_rbf_train, time_rbf_predict))\n",
    "print(classification_report(test_labels, prediction_rbf))\n",
    "print(\"Results for SVC(kernel=linear)\")\n",
    "print(\"Training time: %fs; Prediction time: %fs\" % (time_linear_train, time_linear_predict))\n",
    "print(classification_report(test_labels, prediction_linear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
