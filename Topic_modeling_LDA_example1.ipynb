{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is a document on Topic Modeling**\n",
    "\n",
    "https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\"\n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Cleaning**\n",
    "\n",
    "Data cleaning is absolutely crucial for generating a useful topic model. The steps below are common to most natural language processing methods:\n",
    "\n",
    "Tokenizing: converting a document to its atomic elements.\n",
    "Stopping: removing meaningless words.\n",
    "Stemming: merging words that are equivalent in meaning.\n",
    "\n",
    "** Tokenization**\n",
    "\n",
    "Tokenization can be performed many ways. Here, we are using NLTKâ€™s tokenize.regexp module. Another way is to use \"CountVectorizer\" to \"convert text into a matrix of token counts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# The following code matches any word characters until it reaches a non-word character, like a space. \n",
    "# This is a simple solution, but can cause problems for words like \"don't\" which will be read as two tokens, \"don\" and \"t\".\n",
    "# NLK provides a number of pre-constructed tokenizers like nltk.tokenize.simple. \n",
    "# Its better to use regex and iterate until your document is accurately tokenized. \n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create English stop words list\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "en_stop = get_stop_words('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For example, \"stemming\", \"stemmer\", and \"stemmed\" all have similar meanings;\n",
    "# stemming reduces those terms to \"stem\".\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list of all stemmed tokens\n",
    "texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    #clean and tokenize document string\n",
    "    # convert the document to lower-case\n",
    "    raw = i.lower()\n",
    "    # tokens is a list containing each word in the document\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    \n",
    "    # remove stop words from tokens, words such as \"that\", \"is\", \"for\", \"your\" are removed\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens, \"professionals\" are replaced with \"professional\"\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # texts is a list of temmed_token lists\n",
    "    texts.append(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.083*\"mother\" + 0.083*\"brother\" + 0.083*\"drive\"'), (1, u'0.031*\"mother\" + 0.031*\"brother\" + 0.031*\"drive\"'), (2, u'0.105*\"good\" + 0.105*\"brocolli\" + 0.105*\"health\"')]\n"
     ]
    }
   ],
   "source": [
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word = dictionary, passes=20)\n",
    "\n",
    "# examine the results\n",
    "#num_words: number of words in every topic\n",
    "print(ldamodel.print_topics(num_topics=3, num_words=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.094*\"health\" + 0.066*\"drive\" + 0.065*\"pressur\" + 0.036*\"tension\"'), (1, u'0.107*\"brocolli\" + 0.107*\"good\" + 0.083*\"brother\" + 0.083*\"mother\"')]\n"
     ]
    }
   ],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)\n",
    "\n",
    "print(ldamodel.print_topics(num_topics=2, num_words=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
